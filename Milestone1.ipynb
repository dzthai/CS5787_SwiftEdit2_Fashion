{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dzthai/CS5787_SwiftEdit2_Fashion/blob/main/Milestone1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2kgNmtbAcz16"
      },
      "outputs": [],
      "source": [
        "# Development outline\n",
        "\n",
        "# 1. Get SwiftEdit2 working (4.1)\n",
        "# 2. Prepare / store data (David)\n",
        "    # need to get (4.4) (image, text edit, output image) probably from deepFashion dataset\n",
        "# 3. Train model (Sona)\n",
        "# 4. Evaluate model\n",
        "\n",
        "# Later: Domain specific edits and LoRA Adapters\n",
        "# Writing outline\n",
        "# 1."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "uIEMZ1s0OvoR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MagicBrushFashionLoader:\n",
        "    def __init__(self, base_dir=\"./magicbrush_data\"):\n",
        "        \"\"\"\n",
        "        Initialize the MagicBrush loader\n",
        "\n",
        "        Args:\n",
        "            base_dir: Directory to store downloaded data\n",
        "        \"\"\"\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Fashion-related keywords for filtering\n",
        "        self.fashion_keywords = [\n",
        "            'dress', 'shirt', 'pants', 'jeans', 'coat', 'jacket',\n",
        "            'skirt', 'blouse', 'sweater', 'hoodie', 'clothing',\n",
        "            'fashion', 'wear', 'outfit', 'suit', 'tie', 'shoes',\n",
        "            'boots', 'sneakers', 'hat', 'scarf', 'gloves', 'socks',\n",
        "            'shorts', 'tank top', 't-shirt', 'blazer', 'cardigan',\n",
        "            'uniform', 'robe', 'gown', 'vest', 'sleeve', 'collar',\n",
        "            'button', 'zipper', 'pocket', 'fabric', 'textile',\n",
        "            'cotton', 'silk', 'denim', 'leather', 'wool'\n",
        "        ]\n",
        "\n",
        "    def download_dataset(self):\n",
        "        \"\"\"\n",
        "        Download MagicBrush dataset from HuggingFace\n",
        "        Note: You may need to manually download if this doesn't work\n",
        "        \"\"\"\n",
        "        print(\"Attempting to download MagicBrush dataset...\")\n",
        "        print(\"If this fails, please manually download from:\")\n",
        "        print(\"https://huggingface.co/datasets/osunlp/MagicBrush\")\n",
        "\n",
        "        # Using HuggingFace datasets library\n",
        "        try:\n",
        "            from datasets import load_dataset\n",
        "            dataset = load_dataset(\"osunlp/MagicBrush\")\n",
        "            print(\"‚úì Dataset downloaded successfully!\")\n",
        "            return dataset\n",
        "        except Exception as e:\n",
        "            print(f\"‚úó Download failed: {e}\")\n",
        "            print(\"Please install: pip install datasets\")\n",
        "            return None\n",
        "\n",
        "    def load_from_huggingface(self):\n",
        "        \"\"\"\n",
        "        Load MagicBrush directly from HuggingFace (recommended)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            from datasets import load_dataset\n",
        "            print(\"Loading MagicBrush from HuggingFace...\")\n",
        "            dataset = load_dataset(\"osunlp/MagicBrush\")\n",
        "            print(f\"‚úì Loaded {len(dataset['train'])} training samples\")\n",
        "            return dataset\n",
        "        except ImportError:\n",
        "            print(\"Please install: pip install datasets\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading dataset: {e}\")\n",
        "            return None\n",
        "\n",
        "    def is_fashion_related(self, text):\n",
        "        \"\"\"\n",
        "        Check if instruction text is fashion-related\n",
        "\n",
        "        Args:\n",
        "            text: Instruction text to check\n",
        "\n",
        "        Returns:\n",
        "            bool: True if fashion-related\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        return any(keyword in text_lower for keyword in self.fashion_keywords)\n",
        "\n",
        "    def filter_fashion_samples(self, dataset):\n",
        "        \"\"\"\n",
        "        Filter dataset for fashion-related samples\n",
        "\n",
        "        Args:\n",
        "            dataset: HuggingFace dataset object\n",
        "\n",
        "        Returns:\n",
        "            list: Filtered fashion samples with metadata\n",
        "        \"\"\"\n",
        "        print(\"\\nFiltering for fashion-related images...\")\n",
        "        fashion_samples = []\n",
        "\n",
        "        # Process train split\n",
        "        for idx, sample in enumerate(dataset['train']):\n",
        "            instruction = sample['instruction']\n",
        "\n",
        "            if self.is_fashion_related(instruction):\n",
        "                fashion_samples.append({\n",
        "                    'id': f\"train_{idx}\",\n",
        "                    'source_img': sample['source_img'],\n",
        "                    'target_img': sample['target_img'],\n",
        "                    'instruction': instruction,\n",
        "                    'turn_index': sample.get('turn_index', 0),\n",
        "                    'split': 'train'\n",
        "                })\n",
        "\n",
        "        # Process test split if exists\n",
        "        if 'test' in dataset:\n",
        "            for idx, sample in enumerate(dataset['test']):\n",
        "                instruction = sample['instruction']\n",
        "\n",
        "                if self.is_fashion_related(instruction):\n",
        "                    fashion_samples.append({\n",
        "                        'id': f\"test_{idx}\",\n",
        "                        'source_img': sample['source_img'],\n",
        "                        'target_img': sample['target_img'],\n",
        "                        'instruction': instruction,\n",
        "                        'turn_index': sample.get('turn_index', 0),\n",
        "                        'split': 'test'\n",
        "                    })\n",
        "\n",
        "        print(f\"‚úì Found {len(fashion_samples)} fashion-related samples\")\n",
        "        return fashion_samples\n",
        "\n",
        "    def analyze_fashion_data(self, fashion_samples):\n",
        "        \"\"\"\n",
        "        Analyze the filtered fashion dataset\n",
        "\n",
        "        Args:\n",
        "            fashion_samples: List of fashion samples\n",
        "\n",
        "        Returns:\n",
        "            dict: Statistics about the dataset\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"FASHION DATASET ANALYSIS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Basic statistics\n",
        "        total_samples = len(fashion_samples)\n",
        "        print(f\"\\nüìä Total Fashion Samples: {total_samples}\")\n",
        "\n",
        "        # Split distribution\n",
        "        splits = Counter([s['split'] for s in fashion_samples])\n",
        "        print(f\"\\nüìÇ Split Distribution:\")\n",
        "        for split, count in splits.items():\n",
        "            print(f\"   {split}: {count} samples ({count/total_samples*100:.1f}%)\")\n",
        "\n",
        "        # Instruction analysis\n",
        "        instructions = [s['instruction'] for s in fashion_samples]\n",
        "        avg_length = sum(len(inst.split()) for inst in instructions) / len(instructions)\n",
        "        print(f\"\\nüìù Instruction Statistics:\")\n",
        "        print(f\"   Average length: {avg_length:.1f} words\")\n",
        "\n",
        "        # Keyword frequency\n",
        "        keyword_counts = Counter()\n",
        "        for sample in fashion_samples:\n",
        "            text = sample['instruction'].lower()\n",
        "            for keyword in self.fashion_keywords:\n",
        "                if keyword in text:\n",
        "                    keyword_counts[keyword] += 1\n",
        "\n",
        "        print(f\"\\nüè∑Ô∏è  Top 10 Fashion Keywords:\")\n",
        "        for keyword, count in keyword_counts.most_common(10):\n",
        "            print(f\"   {keyword}: {count} occurrences\")\n",
        "\n",
        "        # Sample instructions\n",
        "        print(f\"\\nüí¨ Sample Instructions:\")\n",
        "        for i, sample in enumerate(fashion_samples[:5]):\n",
        "            print(f\"   {i+1}. \\\"{sample['instruction']}\\\"\")\n",
        "\n",
        "        # Storage estimation\n",
        "        print(f\"\\nüíæ Storage Estimation:\")\n",
        "        # Assuming ~500KB per image pair (conservative estimate)\n",
        "        estimated_size_mb = (total_samples * 2 * 0.5)  # 2 images per sample\n",
        "        print(f\"   Estimated size: ~{estimated_size_mb:.1f} MB\")\n",
        "        print(f\"   Estimated size: ~{estimated_size_mb/1024:.2f} GB\")\n",
        "\n",
        "        stats = {\n",
        "            'total_samples': total_samples,\n",
        "            'splits': dict(splits),\n",
        "            'avg_instruction_length': avg_length,\n",
        "            'top_keywords': keyword_counts.most_common(10),\n",
        "            'estimated_size_mb': estimated_size_mb\n",
        "        }\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def save_fashion_metadata(self, fashion_samples, output_path=\"fashion_samples.json\"):\n",
        "        \"\"\"\n",
        "        Save filtered fashion samples metadata to JSON\n",
        "\n",
        "        Args:\n",
        "            fashion_samples: List of fashion samples\n",
        "            output_path: Path to save JSON file\n",
        "        \"\"\"\n",
        "        output_file = self.base_dir / output_path\n",
        "\n",
        "        # Convert PIL images to paths/info (can't serialize images directly)\n",
        "        serializable_samples = []\n",
        "        for sample in fashion_samples:\n",
        "            serializable_samples.append({\n",
        "                'id': sample['id'],\n",
        "                'instruction': sample['instruction'],\n",
        "                'turn_index': sample['turn_index'],\n",
        "                'split': sample['split'],\n",
        "                'has_source_img': sample['source_img'] is not None,\n",
        "                'has_target_img': sample['target_img'] is not None\n",
        "            })\n",
        "\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(serializable_samples, f, indent=2)\n",
        "\n",
        "        print(f\"\\n‚úì Saved metadata to {output_file}\")\n",
        "        return output_file\n",
        "\n",
        "    def create_sample_visualization(self, fashion_samples, num_samples=3):\n",
        "        \"\"\"\n",
        "        Create a visualization of sample fashion edits\n",
        "\n",
        "        Args:\n",
        "            fashion_samples: List of fashion samples\n",
        "            num_samples: Number of samples to visualize\n",
        "        \"\"\"\n",
        "        print(f\"\\nüñºÔ∏è  Creating visualization of {num_samples} samples...\")\n",
        "\n",
        "        try:\n",
        "            import matplotlib.pyplot as plt\n",
        "\n",
        "            fig, axes = plt.subplots(num_samples, 2, figsize=(10, 5*num_samples))\n",
        "            if num_samples == 1:\n",
        "                axes = axes.reshape(1, -1)\n",
        "\n",
        "            for i in range(min(num_samples, len(fashion_samples))):\n",
        "                sample = fashion_samples[i]\n",
        "\n",
        "                # Source image\n",
        "                axes[i, 0].imshow(sample['source_img'])\n",
        "                axes[i, 0].set_title(f\"Source Image {i+1}\")\n",
        "                axes[i, 0].axis('off')\n",
        "\n",
        "                # Target image\n",
        "                axes[i, 1].imshow(sample['target_img'])\n",
        "                axes[i, 1].set_title(f\"Target Image {i+1}\\n\\\"{sample['instruction']}\\\"\",\n",
        "                                    fontsize=9)\n",
        "                axes[i, 1].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            viz_path = self.base_dir / \"sample_visualization.png\"\n",
        "            plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n",
        "            print(f\"‚úì Saved visualization to {viz_path}\")\n",
        "            plt.show()\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è  matplotlib not installed, skipping visualization\")\n",
        "            print(\"   Install with: pip install matplotlib\")"
      ],
      "metadata": {
        "id": "8M5aqNXgO64u"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"MAGICBRUSH FASHION DATASET LOADER\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Use Colab's temporary storage (no Drive needed!)\n",
        "    base_dir = \"/content/magicbrush_data\"\n",
        "    print(f\"‚úì Using temporary storage: {base_dir}\")\n",
        "    print(\"‚ö†Ô∏è  Note: Data will be deleted when runtime disconnects\")\n",
        "\n",
        "    # Initialize loader\n",
        "    loader = MagicBrushFashionLoader(base_dir=base_dir)\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = loader.load_from_huggingface()\n",
        "\n",
        "    if dataset is None:\n",
        "        print(\"\\n‚ùå Failed to load dataset. Please check your internet connection\")\n",
        "        print(\"   and ensure 'datasets' library is installed.\")\n",
        "        return\n",
        "\n",
        "    # Filter for fashion samples\n",
        "    fashion_samples = loader.filter_fashion_samples(dataset)\n",
        "\n",
        "    if len(fashion_samples) == 0:\n",
        "        print(\"\\n‚ö†Ô∏è  No fashion samples found. Check your keywords.\")\n",
        "        return\n",
        "\n",
        "    # Analyze the data\n",
        "    stats = loader.analyze_fashion_data(fashion_samples)\n",
        "\n",
        "    # Save metadata\n",
        "    loader.save_fashion_metadata(fashion_samples)\n",
        "\n",
        "    # Create visualization\n",
        "    loader.create_sample_visualization(fashion_samples, num_samples=3)\n",
        "\n",
        "    # Recommendations\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RECOMMENDATIONS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if stats['estimated_size_mb'] < 500:  # Less than 500MB\n",
        "        print(\"‚úì Dataset is small enough for Google Drive\")\n",
        "        print(\"  No need for AWS S3 for this project\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Dataset is fairly large\")\n",
        "        print(\"  Consider AWS S3 if you have storage issues\")\n",
        "\n",
        "    print(f\"\\nüìà For your milestone, you have {stats['total_samples']} fashion samples\")\n",
        "    if stats['total_samples'] < 100:\n",
        "        print(\"   ‚ö†Ô∏è  This might be too few. Consider:\")\n",
        "        print(\"   1. Expanding keywords\")\n",
        "        print(\"   2. Using full MagicBrush + augmenting with DeepFashion2\")\n",
        "    elif stats['total_samples'] < 500:\n",
        "        print(\"   ‚úì Good for preliminary results\")\n",
        "        print(\"   ‚úì May want more data for final project\")\n",
        "    else:\n",
        "        print(\"   ‚úì Excellent amount for training!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Required installations\n",
        "    print(\"Required packages:\")\n",
        "    print(\"  pip install datasets pillow pandas matplotlib\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffQh47V4PMSw",
        "outputId": "0790c13e-3f72-40b8-b6a5-691c2d8980c7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required packages:\n",
            "  pip install datasets pillow pandas matplotlib\n",
            "\n",
            "\n",
            "============================================================\n",
            "MAGICBRUSH FASHION DATASET LOADER\n",
            "============================================================\n",
            "‚úì Using temporary storage: /content/magicbrush_data\n",
            "‚ö†Ô∏è  Note: Data will be deleted when runtime disconnects\n",
            "Loading MagicBrush from HuggingFace...\n",
            "Error loading dataset: [Errno 107] Transport endpoint is not connected: 'osunlp/MagicBrush/state.json'\n",
            "\n",
            "‚ùå Failed to load dataset. Please check your internet connection\n",
            "   and ensure 'datasets' library is installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Prepare / Store Data (David)\n",
        "\n",
        "#test\n",
        "class FashionEditDataset(Dataset):\n",
        "    def __init__(self, magicbrush_path, deepfashion_path=None):\n",
        "        # Load MagicBrush data\n",
        "        self.data = self.load_magicbrush(magicbrush_path)\n",
        "\n",
        "        # Optionally augment with DeepFashion2\n",
        "        if deepfashion_path:\n",
        "            self.data.extend(self.load_deepfashion(deepfashion_path))\n",
        "\n",
        "    def load_magicbrush(self, path):\n",
        "        # MagicBrush format is already (source, instruction, target)\n",
        "        samples = []\n",
        "        with open(os.path.join(path, 'annotations.json')) as f:\n",
        "            data = json.load(f)\n",
        "            for item in data:\n",
        "                if self.is_fashion_related(item['instruction']):\n",
        "                    samples.append({\n",
        "                        'original': item['source_img'],\n",
        "                        'edited': item['target_img'],\n",
        "                        'prompt': item['instruction']\n",
        "                    })\n",
        "        return samples"
      ],
      "metadata": {
        "id": "CdUQ9x4WIE4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "OcPMWuokXQHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l0s3yOsIYOPc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}